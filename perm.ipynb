{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = []\n",
    "train_y = []\n",
    "\n",
    "with open('perm_train.txt') as file:\n",
    "    for line in file:\n",
    "        data = line.split()\n",
    "        train_y.append(int(data.pop()))\n",
    "        train_X.append(list(map(int, data)))\n",
    "\n",
    "test_X = []\n",
    "test_y = []\n",
    "\n",
    "with open('perm_test.txt') as file:\n",
    "    for line in file:\n",
    "        data = line.split()\n",
    "        test_y.append(int(data.pop()))\n",
    "        test_X.append(list(map(int, data)))\n",
    "\n",
    "train_X = torch.tensor(train_X, dtype=torch.float)\n",
    "train_y = torch.tensor(train_y, dtype=torch.long)\n",
    "test_X = torch.tensor(test_X, dtype=torch.float)\n",
    "test_y = torch.tensor(test_y, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X -= torch.mean(train_X, dim=0)\n",
    "test_X -= torch.mean(test_X, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = []\n",
    "for i in range(1000):\n",
    "    data_train.append((train_X[i], train_y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 2.3020e+00, -8.1700e-01, -2.0560e+00,  3.7700e-01, -5.7890e+00,\n",
       "         -7.8720e+00, -2.9590e+00, -2.8220e+00,  3.1330e+00,  4.1730e+00,\n",
       "          2.3570e+00,  2.2500e-01, -2.9260e+00,  4.0520e+00,  1.0340e+00,\n",
       "         -6.8290e+00,  1.2020e+00, -9.0900e-01,  4.9450e+00,  7.1690e+00,\n",
       "          3.1620e+00,  3.1520e+00,  4.2330e+00, -6.1510e+00, -2.7850e+00,\n",
       "          3.9760e+00,  4.2700e+00,  3.2910e+00,  3.6500e-01, -7.3600e-01,\n",
       "          6.9540e+00, -1.8510e+00,  4.0760e+00,  2.1860e+00,  1.2740e+00,\n",
       "         -4.7600e+00,  7.2070e+00, -8.2000e-01, -5.9620e+00,  4.2470e+00,\n",
       "          4.9430e+00,  5.3300e+00,  1.2720e+00,  2.1490e+00,  1.4090e+00,\n",
       "          2.0480e+00,  6.1430e+00, -8.7200e-01, -1.7430e+00, -8.0150e+00,\n",
       "         -3.0450e+00, -1.9520e+00,  4.1740e+00,  2.0710e+00,  1.3130e+00,\n",
       "          1.1120e+00,  6.1950e+00, -3.8750e+00, -2.8280e+00, -3.0970e+00,\n",
       "         -8.3500e-01,  3.0960e+00,  3.9890e+00, -2.8220e+00, -1.9510e+00,\n",
       "          3.0910e+00, -5.0580e+00,  2.0300e+00,  3.1650e+00,  3.2160e+00,\n",
       "          2.2890e+00,  1.1360e+00, -3.8180e+00,  1.2280e+00,  4.2190e+00,\n",
       "          1.7800e-01,  2.1000e-01, -2.9320e+00, -1.8200e+00, -8.7100e-01,\n",
       "          4.1660e+00,  2.9540e+00, -8.7100e-01, -6.6100e-01, -2.0490e+00,\n",
       "         -1.9470e+00,  2.8100e-01, -6.8790e+00, -4.8850e+00, -5.9130e+00,\n",
       "          2.0430e+00,  1.8000e-02,  4.9890e+00, -2.8360e+00, -2.1960e+00,\n",
       "          1.1140e+01,  2.2080e+00,  1.2300e-01, -2.8190e+00,  1.4600e-01,\n",
       "          2.7400e-01, -3.6870e+00, -7.6300e-01, -2.9650e+00, -1.8270e+00,\n",
       "          6.9400e+00, -1.9640e+00,  6.3900e+00, -2.9110e+00,  2.3200e-01,\n",
       "          4.1570e+00,  4.0540e+00, -1.8580e+00, -3.8520e+00, -6.8350e+00,\n",
       "         -6.8960e+00, -4.8670e+00, -7.7600e-01, -1.8820e+00, -5.8320e+00,\n",
       "         -5.7540e+00,  2.0520e+00, -4.0240e+00, -6.4500e-01,  7.2110e+00,\n",
       "         -7.3600e-01, -7.0000e-03,  2.4700e-01, -1.9300e+00, -2.9690e+00,\n",
       "         -3.8360e+00,  4.1580e+00, -1.9940e+00,  1.0320e+00,  3.0350e+00,\n",
       "          1.0730e+00, -4.1690e+00,  4.0370e+00, -1.8110e+00,  2.1120e+00,\n",
       "         -1.9400e+00,  2.4100e-01, -3.9120e+00, -2.7860e+00, -4.6710e+00,\n",
       "          1.3630e+00,  2.1600e-01, -2.9010e+00, -7.6200e-01, -1.5950e+00,\n",
       "          1.1260e+00, -7.7210e+00,  5.3910e+00,  8.1930e+00, -3.1410e+00,\n",
       "         -5.8970e+00,  4.1770e+00,  1.3000e-01,  3.2650e+00,  6.0140e+00,\n",
       "         -7.7700e-01,  4.1050e+00,  6.1470e+00, -7.6740e+00,  3.2370e+00,\n",
       "          7.4320e+00,  1.0660e+00, -2.6740e+00, -4.4000e-02,  3.1640e+00,\n",
       "         -6.8000e-02,  1.0630e+00, -1.7870e+00, -1.8310e+00,  5.2110e+00,\n",
       "         -1.6660e+00, -4.9810e+00, -4.9520e+00, -3.7610e+00,  4.9190e+00,\n",
       "          1.1100e-01,  2.2520e+00, -8.8700e-01, -2.7590e+00,  5.4810e+00,\n",
       "         -1.6580e+00, -7.9970e+00,  3.5600e-01, -7.6640e+00,  2.1380e+00,\n",
       "          4.0370e+00,  7.7000e-02, -9.1200e-01,  3.2400e+00, -2.9730e+00,\n",
       "          1.1100e-01,  2.1790e+00, -7.7800e-01,  1.4500e-01,  2.5700e-01,\n",
       "          2.2130e+00,  3.1180e+00, -1.8170e+00,  1.2700e+00,  6.9520e+00,\n",
       "          2.2770e+00,  4.0870e+00, -5.0000e-02, -1.8740e+00,  4.2160e+00,\n",
       "         -5.9070e+00,  4.2700e-01, -9.0800e-01,  3.1490e+00,  9.9890e+00,\n",
       "          2.0100e-01, -9.9500e-01,  2.9720e+00,  3.0770e+00, -4.0410e+00,\n",
       "         -4.8420e+00, -3.9250e+00, -4.0290e+00, -1.9980e+00,  1.2320e+00,\n",
       "          1.1890e+00,  2.3510e+00, -6.7800e-01, -5.8250e+00, -3.9570e+00,\n",
       "         -7.6810e+00, -1.9230e+00,  3.0220e+00,  5.1290e+00, -3.0760e+00,\n",
       "         -8.1300e-01, -2.8960e+00,  3.2310e+00, -2.9310e+00,  3.2540e+00,\n",
       "         -8.7250e+00,  1.2000e-01, -9.8590e+00, -9.6200e-01, -4.0460e+00,\n",
       "          9.0930e+00,  5.1940e+00, -8.5500e-01, -7.9630e+00, -2.7560e+00,\n",
       "         -3.0030e+00, -3.8020e+00,  1.0319e+01, -3.9350e+00, -9.8000e-02,\n",
       "         -3.7870e+00, -7.7900e-01,  5.2660e+00,  2.0430e+00,  1.7700e-01,\n",
       "         -6.8700e-01,  2.2260e+00, -1.0032e+01, -9.1300e-01, -1.7990e+00,\n",
       "         -2.8920e+00,  1.2020e+00,  2.0560e+00,  7.0140e+00,  6.2610e+00,\n",
       "          3.9770e+00,  1.0350e+00,  4.3050e+00, -2.7130e+00, -6.8240e+00,\n",
       "         -7.9150e+00,  2.1500e-01,  3.3500e+00, -2.0050e+00,  4.1520e+00,\n",
       "          3.0390e+00,  3.0060e+00, -2.7940e+00, -1.9070e+00, -5.8200e+00,\n",
       "         -5.7360e+00,  7.0770e+00,  2.1200e+00,  2.1080e+00, -2.7920e+00,\n",
       "          2.0970e+00,  1.0070e+00, -2.9610e+00,  4.9930e+00,  2.0990e+00,\n",
       "          3.0760e+00,  2.0700e-01, -3.0470e+00, -1.1180e+00,  2.0610e+00,\n",
       "          7.1310e+00,  1.0790e+00,  6.1810e+00,  1.9610e+00, -9.3100e-01,\n",
       "         -5.0500e+00,  3.8950e+00,  4.0710e+00,  1.7300e-01,  2.5000e-02,\n",
       "          3.1740e+00,  2.1730e+00,  7.2000e-02,  4.0940e+00,  2.2330e+00,\n",
       "         -3.9990e+00,  1.1930e+00,  2.2670e+00, -1.9820e+00, -3.9660e+00,\n",
       "          4.0940e+00, -3.8100e+00,  2.6600e-01,  4.2540e+00,  8.0760e+00,\n",
       "         -3.9680e+00, -4.8960e+00, -1.7850e+00, -5.8320e+00,  1.3000e+00,\n",
       "          2.8520e+00,  4.1390e+00, -1.7750e+00, -6.8790e+00, -1.0998e+01,\n",
       "         -8.2000e-01,  6.8000e+00, -3.1280e+00,  5.0450e+00, -5.7460e+00,\n",
       "          6.1950e+00,  3.2760e+00,  3.2610e+00,  6.1190e+00, -1.0680e+00,\n",
       "         -6.4000e-02, -3.9420e+00, -1.9440e+00, -3.7880e+00, -9.7800e-01,\n",
       "         -5.6640e+00,  2.0930e+00,  4.1900e+00,  3.9300e-01,  8.0590e+00,\n",
       "         -5.8150e+00,  1.5600e-01,  1.5600e-01,  3.1770e+00,  2.9400e-01,\n",
       "          5.1180e+00,  5.0210e+00,  2.0490e+00, -2.6990e+00,  4.7800e-01,\n",
       "         -8.9800e-01, -1.8120e+00,  2.2400e-01, -7.2800e-01,  7.0190e+00,\n",
       "         -4.8640e+00, -7.7260e+00, -8.1800e-01, -3.8800e+00, -1.1650e+00,\n",
       "          1.0110e+00,  4.3220e+00,  3.3550e+00,  9.1000e-01, -4.8280e+00,\n",
       "         -2.8930e+00, -2.6530e+00, -2.8300e+00, -9.8090e+00,  1.5100e-01,\n",
       "         -2.9830e+00, -7.6900e-01, -5.0160e+00, -1.7380e+00, -8.8100e-01,\n",
       "          5.1010e+00, -1.8660e+00, -2.8780e+00,  8.4000e-02,  2.2700e+00,\n",
       "         -1.8900e+00, -2.8550e+00,  9.2030e+00,  1.1700e-01,  1.7100e-01,\n",
       "          5.2000e-02,  1.3800e-01,  1.3300e-01, -5.6390e+00,  8.0550e+00,\n",
       "         -4.8680e+00,  3.1500e+00,  1.1000e+00,  1.1710e+00, -4.6600e+00,\n",
       "         -8.3900e-01, -1.8900e+00,  2.2300e-01,  1.1760e+00,  3.8780e+00,\n",
       "          1.0300e-01, -3.7590e+00,  4.0640e+00,  4.0610e+00,  5.3850e+00,\n",
       "         -4.8670e+00, -6.7500e-01,  3.1610e+00, -6.9000e-01, -1.8880e+00,\n",
       "         -2.7790e+00,  1.5400e-01,  1.8600e-01,  1.1710e+00,  3.1200e+00,\n",
       "         -3.0720e+00, -8.1100e-01,  1.8970e+00,  1.3080e+00,  9.0600e+00,\n",
       "         -1.9480e+00, -4.5800e+00, -2.9110e+00,  3.1980e+00, -1.9010e+00,\n",
       "         -2.7710e+00, -1.9300e+00,  2.2030e+00, -3.7100e+00,  5.1610e+00,\n",
       "         -8.8700e-01, -2.3000e-02, -9.5000e-01,  2.0400e-01, -6.0000e-02,\n",
       "          4.3000e-02, -4.7590e+00, -4.7860e+00,  9.8000e-01, -1.9310e+00,\n",
       "         -5.8920e+00,  7.1000e+00,  5.1370e+00,  9.4300e-01, -2.0000e+00,\n",
       "         -3.8720e+00,  4.0470e+00,  3.2490e+00,  2.8310e+00,  3.1180e+00,\n",
       "          2.2660e+00, -8.7800e-01, -4.9820e+00, -4.4000e-02,  2.2790e+00,\n",
       "          9.8200e-01,  5.1820e+00, -8.3500e-01, -3.8430e+00, -7.7700e-01,\n",
       "         -2.5910e+00, -3.0830e+00,  3.3600e+00, -2.5530e+00, -4.9620e+00,\n",
       "         -4.9570e+00,  9.7000e-01,  2.9810e+00, -2.4170e+00, -1.7490e+00,\n",
       "          3.5500e-01,  1.9930e+00,  6.0340e+00,  2.2400e-01, -8.8200e-01,\n",
       "          7.1590e+00, -2.8170e+00,  7.0740e+00, -5.9840e+00,  3.0620e+00,\n",
       "          8.3000e-02, -2.6000e-02, -1.0830e+00, -2.8020e+00, -2.9960e+00,\n",
       "         -1.0913e+01, -6.8770e+00,  3.1520e+00,  1.0250e+00, -1.5450e+00,\n",
       "         -9.6210e+00,  1.9810e+00,  1.1430e+00, -7.8270e+00, -6.0530e+00,\n",
       "          4.1950e+00, -1.8690e+00,  4.2350e+00,  3.2660e+00,  3.0340e+00,\n",
       "         -1.0120e+00, -2.8490e+00,  1.3500e-01, -8.0620e+00, -2.9760e+00,\n",
       "          3.0130e+00,  8.9540e+00, -4.0180e+00,  2.0890e+00,  6.4330e+00,\n",
       "         -1.5450e+00,  2.0880e+00,  4.1150e+00,  9.0280e+00,  2.2200e-01,\n",
       "          1.1042e+01, -9.3600e-01,  1.0540e+00,  3.0220e+00, -3.8150e+00,\n",
       "         -6.9900e-01, -1.1000e-02,  1.0870e+00,  1.1700e+00,  3.0360e+00,\n",
       "         -8.2900e-01, -1.8920e+00, -5.8580e+00,  3.1700e-01,  2.8290e+00,\n",
       "          1.1220e+00,  7.1610e+00,  7.4000e-02, -3.2100e+00, -2.7950e+00,\n",
       "          1.6600e-01,  6.0280e+00, -5.8990e+00,  5.0890e+00,  1.2890e+00,\n",
       "          8.3030e+00,  1.1000e-02, -8.6960e+00,  1.2230e+00,  4.9700e+00,\n",
       "          2.0950e+00, -1.7880e+00,  9.3200e-01, -7.8920e+00,  2.3280e+00,\n",
       "          9.7900e-01, -4.9340e+00, -3.7570e+00,  5.3000e-02,  3.1160e+00,\n",
       "          5.1230e+00, -2.8470e+00,  2.9090e+00,  1.0000e+00,  3.3680e+00,\n",
       "         -3.7390e+00,  2.9490e+00,  2.2300e-01,  3.3140e+00,  1.1061e+01,\n",
       "          4.1830e+00,  5.2750e+00,  5.1080e+00, -2.7330e+00, -5.9030e+00,\n",
       "         -1.7710e+00, -3.8690e+00, -2.9970e+00,  7.4390e+00,  1.3920e+00,\n",
       "          4.1230e+00, -6.9270e+00,  6.2990e+00, -2.7980e+00,  4.0790e+00,\n",
       "          4.2220e+00, -4.9490e+00, -3.7940e+00,  2.2520e+00, -9.5000e-01,\n",
       "          2.0000e-01, -6.8900e-01, -7.8600e-01,  1.0060e+00,  3.0760e+00,\n",
       "          4.2570e+00,  5.1200e+00,  5.1990e+00,  6.9930e+00,  5.3570e+00,\n",
       "          4.1250e+00, -9.8000e+00,  3.1710e+00, -8.0940e+00,  4.0740e+00,\n",
       "          2.3080e+00,  2.9050e+00, -4.1540e+00,  4.2090e+00,  8.1610e+00,\n",
       "         -5.8250e+00, -5.0710e+00,  7.3530e+00, -3.0250e+00,  1.1800e-01,\n",
       "         -6.0650e+00,  3.5700e-01,  1.4590e+00, -3.7400e+00, -6.7310e+00,\n",
       "         -1.7660e+00,  7.0000e-02,  4.1440e+00,  1.9690e+00, -4.9350e+00,\n",
       "          1.2600e-01, -3.8480e+00, -3.8660e+00,  1.4440e+00,  3.0500e-01,\n",
       "          1.1180e+00,  1.0730e+00,  7.1340e+00, -2.9160e+00,  5.0300e+00,\n",
       "         -2.8050e+00, -2.7730e+00, -2.7940e+00, -1.7800e-01,  1.1100e+00,\n",
       "         -8.6900e-01,  4.3680e+00,  2.3630e+00, -5.7780e+00, -9.7600e-01,\n",
       "         -2.8780e+00,  1.1540e+00, -1.8390e+00, -1.8520e+00,  1.2320e+00,\n",
       "         -4.1020e+00, -6.8400e-01,  1.2910e+00,  3.2100e+00,  3.1600e+00,\n",
       "          3.1370e+00,  5.1990e+00,  3.9930e+00,  2.4100e-01, -5.3000e-02,\n",
       "         -4.8810e+00,  1.9590e+00,  5.9020e+00,  3.2600e+00,  1.1180e+00,\n",
       "          3.0300e+00,  6.3730e+00,  9.1600e-01,  1.2190e+00, -2.9570e+00,\n",
       "          2.6900e-01,  1.0470e+00,  2.0540e+00,  2.2510e+00, -2.1650e+00,\n",
       "         -4.8260e+00,  1.3270e+00,  3.0230e+00,  5.2160e+00,  8.0680e+00,\n",
       "          3.3500e+00,  5.9100e+00,  5.2450e+00, -4.6170e+00, -1.6520e+00,\n",
       "         -2.9590e+00,  4.1160e+00,  2.1800e+00,  9.0000e-02,  2.1050e+00,\n",
       "         -8.1900e-01,  3.0690e+00,  3.2100e+00,  7.3620e+00,  2.0600e-01,\n",
       "         -7.8700e-01, -3.1300e+00,  2.1680e+00, -3.9790e+00,  1.1860e+00,\n",
       "         -3.7030e+00, -6.7570e+00, -3.7370e+00,  3.2090e+00, -1.0820e+00,\n",
       "          5.1010e+00, -7.7760e+00, -3.6930e+00,  8.2700e+00, -2.0900e-01,\n",
       "          5.5000e-02,  1.2209e+01, -4.9580e+00, -3.8150e+00, -6.7400e+00,\n",
       "          1.2950e+00,  1.0100e-01,  4.1130e+00,  5.7000e-02, -8.8160e+00,\n",
       "         -3.8320e+00, -5.9750e+00, -1.7900e+00, -2.8680e+00,  1.1680e+00,\n",
       "          2.1260e+00, -8.6600e-01, -1.0320e+00,  1.3140e+00, -7.7000e-02,\n",
       "         -4.7890e+00,  2.0530e+00, -4.9190e+00,  3.2910e+00,  1.0520e+00,\n",
       "          6.1070e+00,  1.2450e+00, -3.8560e+00, -3.9520e+00, -2.7160e+00,\n",
       "          2.8490e+00,  2.1470e+00, -9.8900e-01,  8.1860e+00,  2.2220e+00,\n",
       "         -1.5170e+00,  4.3050e+00,  2.2760e+00,  1.2490e+00, -1.4100e-01,\n",
       "          4.0870e+00, -5.9780e+00, -1.8240e+00,  2.2200e+00,  3.2010e+00,\n",
       "          2.3000e-02, -4.1800e+00, -5.9890e+00,  2.2200e+00, -1.8950e+00,\n",
       "         -2.6870e+00,  1.2020e+00,  1.0860e+00,  4.2300e-01, -1.9800e+00,\n",
       "          2.3260e+00, -7.5800e-01,  4.1530e+00,  4.2040e+00,  2.1690e+00,\n",
       "          7.1210e+00,  1.0280e+00, -2.8500e+00,  2.1200e-01, -2.0060e+00,\n",
       "          6.0890e+00, -1.0810e+00,  2.8720e+00,  9.1270e+00, -4.7640e+00,\n",
       "          1.1780e+00,  1.1490e+00, -3.8150e+00,  4.3090e+00,  3.9790e+00,\n",
       "          2.9540e+00, -5.9450e+00,  4.0960e+00, -5.9920e+00, -4.8770e+00,\n",
       "         -1.8740e+00,  1.1000e-02,  1.9100e-01,  1.0710e+00,  1.1000e-01,\n",
       "         -2.8400e+00,  4.2140e+00, -2.0510e+00,  2.2030e+00, -1.9990e+00,\n",
       "         -4.9850e+00,  1.4600e-01, -4.8930e+00,  1.2365e+01, -9.5100e-01,\n",
       "         -2.7840e+00,  4.0250e+00, -1.1821e+01, -6.8410e+00,  7.1500e+00,\n",
       "         -1.8640e+00,  3.1490e+00, -2.7930e+00, -6.0050e+00, -2.1000e-02,\n",
       "          7.3650e+00, -1.8510e+00, -1.0956e+01, -1.9000e-02, -5.6870e+00,\n",
       "         -1.9999e-03, -2.9270e+00, -2.9000e+00,  2.1220e+00, -2.7560e+00,\n",
       "          1.3200e-01, -6.0000e-02, -5.7880e+00, -2.9090e+00, -1.6620e+00,\n",
       "         -3.8610e+00,  3.1890e+00, -5.9450e+00,  9.1790e+00,  7.1470e+00,\n",
       "          6.3120e+00, -2.5770e+00,  4.1530e+00,  2.3980e+00, -4.8170e+00,\n",
       "         -4.9340e+00, -4.7530e+00,  5.1180e+00, -4.8690e+00, -5.8380e+00,\n",
       "          1.4000e-02, -6.8000e+00, -1.9900e+00, -3.6990e+00, -7.0110e+00,\n",
       "          4.1390e+00, -6.0700e-01, -1.9070e+00,  5.1830e+00,  8.5700e-01,\n",
       "          3.9240e+00, -3.8340e+00, -1.7700e+00, -3.8990e+00, -3.4410e+00,\n",
       "         -2.9500e+00,  2.0400e+00, -9.9500e-01, -1.0970e+00,  1.1590e+00,\n",
       "          1.0110e+00, -4.6930e+00,  2.0400e-01, -3.0150e+00, -5.7080e+00,\n",
       "          2.1510e+00, -4.0050e+00, -8.7750e+00,  5.3470e+00, -1.4500e-01,\n",
       "          2.1900e-01,  2.0220e+00,  4.2850e+00, -4.7530e+00, -1.9180e+00,\n",
       "         -2.8730e+00,  3.0330e+00,  2.0920e+00,  9.9000e-01, -5.9370e+00,\n",
       "         -8.9600e-01,  9.1180e+00,  5.2500e+00,  6.1530e+00, -9.1900e-01,\n",
       "          1.3430e+00, -2.8760e+00,  2.0990e+00, -3.9870e+00,  1.4700e-01,\n",
       "         -3.6270e+00,  4.2130e+00, -3.8160e+00, -9.2600e-01,  2.2430e+00,\n",
       "          7.2220e+00, -3.7820e+00,  1.1480e+00,  1.8630e+00, -2.7010e+00,\n",
       "          3.1310e+00,  8.8000e-02, -4.6080e+00,  5.0750e+00, -4.7680e+00,\n",
       "         -2.7770e+00,  4.1860e+00, -4.9990e+00, -1.5680e+00, -3.6800e+00,\n",
       "          2.2090e+00,  2.1800e-01,  2.0200e+00, -1.8420e+00, -4.7510e+00,\n",
       "         -8.9700e-01, -3.8840e+00, -4.8630e+00, -1.9790e+00, -5.7900e+00,\n",
       "          3.9180e+00, -1.8200e+00,  6.0770e+00, -3.8650e+00, -9.7440e+00,\n",
       "         -2.7330e+00,  3.1770e+00,  2.1830e+00,  8.1580e+00,  4.1210e+00,\n",
       "         -5.9590e+00,  5.0520e+00, -8.3800e-01,  1.1650e+00, -6.8710e+00,\n",
       "         -8.9600e-01,  1.0270e+00,  3.2070e+00, -1.8310e+00, -2.8350e+00,\n",
       "          1.2750e+00,  9.9000e-02, -2.9340e+00,  5.1130e+00, -9.8530e+00,\n",
       "          4.1790e+00,  4.2870e+00, -1.8500e+00,  5.2050e+00,  1.1110e+00,\n",
       "          4.0370e+00,  3.1330e+00, -7.3300e-01, -4.7720e+00,  3.3910e+00,\n",
       "         -5.9750e+00,  3.3900e-01,  2.1400e-01, -2.8430e+00,  2.0640e+00,\n",
       "          1.3200e+00,  4.0850e+00, -3.8050e+00, -1.8350e+00,  1.4000e-01,\n",
       "          4.9670e+00, -8.8630e+00,  3.0000e-02,  1.7400e-01,  2.3570e+00,\n",
       "          4.1490e+00,  6.0780e+00,  7.1720e+00,  6.0500e+00, -5.2160e+00,\n",
       "         -6.8550e+00, -4.6590e+00,  7.2700e+00,  1.1480e+00, -5.1060e+00,\n",
       "         -8.6500e-01, -2.5380e+00,  5.1490e+00, -8.2000e-01, -9.9780e+00]),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "\n",
    "data_size = 1000\n",
    "validation_split = .2\n",
    "split = int(validation_split * data_size)\n",
    "indices = np.arange(data_size)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "val_loader = torch.utils.data.DataLoader(data_train, batch_size=batch_size,\n",
    "                                         sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "sls = 100\n",
    "nn_model = nn.Sequential(\n",
    "            nn.Linear(1000, sls),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(sls, sls),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(sls, 2), \n",
    "         )\n",
    "nn_model.type(torch.FloatTensor)\n",
    "\n",
    "# We will minimize cross-entropy between the ground truth and\n",
    "# network predictions using an SGD optimizer\n",
    "loss = nn.CrossEntropyLoss().type(torch.FloatTensor)\n",
    "optimizer = optim.SGD(nn_model.parameters(), lr=2, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: 0.721501, Train accuracy: 0.481250, Val accuracy: 0.525000\n",
      "Average loss: 2.357935, Train accuracy: 0.505000, Val accuracy: 0.475000\n",
      "Average loss: 14.796508, Train accuracy: 0.495000, Val accuracy: 0.475000\n",
      "Average loss: 0.800126, Train accuracy: 0.495000, Val accuracy: 0.475000\n",
      "Average loss: 0.702538, Train accuracy: 0.495000, Val accuracy: 0.475000\n",
      "Average loss: 0.695843, Train accuracy: 0.495000, Val accuracy: 0.475000\n",
      "Average loss: 0.689957, Train accuracy: 0.497500, Val accuracy: 0.475000\n",
      "Average loss: 0.683440, Train accuracy: 0.526250, Val accuracy: 0.550000\n",
      "Average loss: 0.654913, Train accuracy: 0.740000, Val accuracy: 0.605000\n",
      "Average loss: 0.599556, Train accuracy: 0.863750, Val accuracy: 0.665000\n",
      "Average loss: 0.507157, Train accuracy: 0.901250, Val accuracy: 0.690000\n",
      "Average loss: 0.467553, Train accuracy: 0.912500, Val accuracy: 0.690000\n",
      "Average loss: 0.438441, Train accuracy: 0.915000, Val accuracy: 0.690000\n",
      "Average loss: 0.427165, Train accuracy: 0.921250, Val accuracy: 0.700000\n",
      "Average loss: 0.416809, Train accuracy: 0.921250, Val accuracy: 0.705000\n",
      "Average loss: 0.412007, Train accuracy: 0.923750, Val accuracy: 0.705000\n",
      "Average loss: 0.407334, Train accuracy: 0.923750, Val accuracy: 0.705000\n",
      "Average loss: 0.404968, Train accuracy: 0.923750, Val accuracy: 0.710000\n",
      "Average loss: 0.402609, Train accuracy: 0.925000, Val accuracy: 0.710000\n",
      "Average loss: 0.401411, Train accuracy: 0.925000, Val accuracy: 0.710000\n",
      "Average loss: 0.400172, Train accuracy: 0.926250, Val accuracy: 0.710000\n",
      "Average loss: 0.399543, Train accuracy: 0.926250, Val accuracy: 0.705000\n",
      "Average loss: 0.398898, Train accuracy: 0.926250, Val accuracy: 0.705000\n",
      "Average loss: 0.398573, Train accuracy: 0.926250, Val accuracy: 0.705000\n",
      "Average loss: 0.398246, Train accuracy: 0.926250, Val accuracy: 0.705000\n",
      "Average loss: 0.398084, Train accuracy: 0.926250, Val accuracy: 0.705000\n",
      "Average loss: 0.397923, Train accuracy: 0.926250, Val accuracy: 0.705000\n",
      "Average loss: 0.397842, Train accuracy: 0.926250, Val accuracy: 0.705000\n",
      "Average loss: 0.397760, Train accuracy: 0.926250, Val accuracy: 0.705000\n",
      "Average loss: 0.397719, Train accuracy: 0.926250, Val accuracy: 0.705000\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, val_loader, loss, optimizer, num_epochs):    \n",
    "    loss_history = []\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "    for epoch in range(num_epochs):\n",
    "        scheduler.step()\n",
    "        model.train() # Enter train mode\n",
    "        \n",
    "        loss_accum = 0\n",
    "        correct_samples = 0\n",
    "        total_samples = 0\n",
    "        for i_step, (x, y) in enumerate(train_loader):\n",
    "            prediction = model(x)    \n",
    "            loss_value = loss(prediction, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, indices = torch.max(prediction, 1)\n",
    "            correct_samples += torch.sum(indices == y)\n",
    "            total_samples += y.shape[0]\n",
    "            \n",
    "            loss_accum += loss_value\n",
    "\n",
    "        ave_loss = loss_accum / (i_step + 1)\n",
    "        train_accuracy = float(correct_samples) / total_samples\n",
    "        val_accuracy = compute_accuracy(model, val_loader)\n",
    "        \n",
    "        loss_history.append(float(ave_loss))\n",
    "        train_history.append(train_accuracy)\n",
    "        val_history.append(val_accuracy)\n",
    "        \n",
    "        print(\"Average loss: %f, Train accuracy: %f, Val accuracy: %f\" % (ave_loss, train_accuracy, val_accuracy))\n",
    "        \n",
    "    return loss_history, train_history, val_history\n",
    "        \n",
    "def compute_accuracy(model, loader):\n",
    "    \"\"\"\n",
    "    Computes accuracy on the dataset wrapped in a loader\n",
    "    \n",
    "    Returns: accuracy as a float value between 0 and 1\n",
    "    \"\"\"\n",
    "    model.eval() # Evaluation mode\n",
    "    # TODO: Implement the inference of the model on all of the batches from loader,\n",
    "    #       and compute the overall accuracy.\n",
    "    # Hint: torch doesn't have a dedicated argmax function,\n",
    "    #       but you can use torch.max instead (see the documentation).\n",
    "    \n",
    "    correct_samples = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for x, y in loader:\n",
    "        predictions = model(x)\n",
    "        \n",
    "        _, indices = torch.max(predictions, 1)\n",
    "        \n",
    "        correct_samples += torch.sum(indices == y)\n",
    "        total_samples += y.shape[0]\n",
    "    \n",
    "    accuracy = float(correct_samples) / total_samples\n",
    "    return accuracy\n",
    "\n",
    "loss_history, train_history, val_history = train_model(nn_model, train_loader, val_loader, loss, optimizer, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
